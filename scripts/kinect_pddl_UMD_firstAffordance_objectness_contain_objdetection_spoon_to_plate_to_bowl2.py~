#!/usr/bin/env python
"""
See README.md for installation instructions before running.
Demo script to perform affordace detection from images
"""

from os import path
import sys
import time
import freenect
import math

# Initialize python paths
this_dir = path.dirname(__file__)

# Add caffe to PYTHONPATH
caffe_path = path.join(this_dir, '..', 'caffe-affordance-net', 'python')
if caffe_path not in sys.path:
    sys.path.append(caffe_path)

# Add lib to PYTHONPATH
lib_path = path.join(this_dir, '..', 'lib')
if lib_path not in sys.path:
    sys.path.append(lib_path)

from fast_rcnn.config import cfg
from fast_rcnn.test import im_detect2
from fast_rcnn.nms_wrapper import nms
from utils.timer import Timer
from utils.camera_to_marker import aruco_camPose
from utils.handy_CONTAINABLE_STATE2 import write_pddl

import numpy as np
import os, cv2
import argparse
import caffe
import time
import subprocess

### ROS STUFF
import rospy

# Message type for publishing can be reused
from visualization_msgs.msg import Marker, MarkerArray
from geometry_msgs.msg import Pose, Point, PoseStamped
from sensor_msgs.msg import Image, PointField, PointCloud2
import sensor_msgs.point_cloud2 as pc2
from handy_experiment.msg import action_msg
from std_msgs import msg
import cv_bridge
import matplotlib.pyplot as plt
#from ros_image_io import ImageIO



# start ros node and imageio
rospy.init_node('AffordanceNet_Node')
#pub_obj_pose_3D = rospy.Publisher("vs_obj_pose_3D", PoseStamped) # pose of object in camera frame
pub_point_cloud = rospy.Publisher('transformed_scene', PointCloud2)
pub_action_msg = rospy.Publisher('action_plan', action_msg)
#KINECT_FX = 525
#KINECT_FY = 525
#KINECT_CX = 319.5
#KINECT_CY = 239.5

KINECT_FX = 526.37013657
KINECT_FY = 526.37013657
KINECT_CX = 313.68782938
KINECT_CY = 259.01834898

CONF_THRESHOLD = 0.1
good_range = 0.005
loadstate = True
    
# get current dir
cwd = os.getcwd()
root_path = os.path.abspath(os.path.join(cwd, os.pardir))  # get parent path
print 'AffordanceNet root folder: ', root_path
img_folder = cwd + '/img'

CLASSES = ('__background__', 'bowl', 'cup', 'hammer', 'knife', 'ladle', 'mallet', 'mug', 'pot', 'saw', 'scissors','scoop','shears','shovel','spoon','tenderizer','trowel','turner','computer_mouse','fork','lipstick','pen','pizza_cutter','plate','remote_control','screwdriver','spoon','stapler','wrench')
OBJ_CLASSES = ('__background__', 'objectness')
OBJ_INDS = {'__background__' : 0, 'bowl' : 1, 'cup' : 2, 'hammer' : 3, 'knife' : 4,
            'ladle' : 5, 'mallet' : 6, 'mug' : 7, 'pot' : 8, 'saw' : 9, 'scissors' : 10, 'scoop' : 11,
            'shears' : 12, 'shovel' : 13, 'spoon' : 14, 'tenderizer' : 15, 'trowel' : 16, 'turner' : 17,
            'computer_mouse' :18, 'fork':19, 'lipstick':20, 'pen':21, 'pizza_cutter':22, 'plate':23, 'remote_control': 24,
            'screwdriver':25, 'spoon':14, 'stapler':27, 'wrench':28}

OBJ_INDS_binary = {'__background__' : 0, 'target' : 1, 'tool' : 2 }
ACTION_INDS = {'pickup' : 0, 'dropoff' : 1, 'scoop': 2}

# Mask
background = [200, 222, 250]  
c1 = [0,0,205] #grasp red  
c2 = [34,139,34] #cut green
c3 = [0,255,255] #scoop bluegreen 
c4 = [165,42,42] #contain dark blue   
c5 = [128,64,128] #pound purple 
c6 = [51,153,255] #support orange
c7 = [184,134,11] #wrap-grasp light blue
c8 = [0,153,153]
c9 = [0,134,141]
c10 = [184,0,141] 
c11 = [184,134,0] 
c12 = [184,134,223]
label_colours = np.array([background, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12])

# Object
col0 = [0, 0, 0]
col1 = [0, 255, 255]
col2 = [255, 0, 255]
col3 = [0, 125, 255]
col4 = [55, 125, 0]
col5 = [255, 50, 75]
col6 = [100, 100, 50]
col7 = [25, 234, 54]
col8 = [156, 65, 15]
col9 = [215, 25, 155]
col10 = [25, 25, 155]
col11 = [100, 100, 50]#
col12 = [25, 234, 54]#
col13 = [156, 65, 15]#
col14 = [215, 25, 155]#
col15 = [25, 25, 155]#
col16 = [100, 100, 50]#
col17 = [184,134,11]

col_map = [col0, col1, col2, col3, col4, col5, col6, col7, col8, col9, col10, col11, col12, col13, col14, col15, col16, col17]


# function to get RGB image from kinect
def get_video():
    array, _ = freenect.sync_get_video()
    array = cv2.cvtColor(array, cv2.COLOR_RGB2BGR)
    return array


# function to get depth image from kinect
def get_depth():
    array_raw, _ = freenect.sync_get_depth(format = 4)
    array = array_raw.astype(np.uint8)
    return array, array_raw


def reset_mask_ids(mask, before_uni_ids):
    # reset ID mask values from [0, 1, 4] to [0, 1, 2] to resize later 
    counter = 0
    for id in before_uni_ids:
        mask[mask == id] = counter
        counter += 1
        
    return mask
    

    
def convert_mask_to_original_ids_manual(mask, original_uni_ids):
    #TODO: speed up!!!
    temp_mask = np.copy(mask) # create temp mask to do np.around()
    temp_mask = np.around(temp_mask, decimals=0)  # round 1.6 -> 2., 1.1 -> 1.
    current_uni_ids = np.unique(temp_mask)
     
    out_mask = np.full(mask.shape, 0, 'float32')
     
    mh, mw = mask.shape
    for i in range(mh-1):
        for j in range(mw-1):
            for k in range(1, len(current_uni_ids)):
                if mask[i][j] > (current_uni_ids[k] - good_range) and mask[i][j] < (current_uni_ids[k] + good_range):  
                    out_mask[i][j] = original_uni_ids[k] 
                    #mask[i][j] = current_uni_ids[k]
           
#     const = 0.005
#     out_mask = original_uni_ids[(np.abs(mask - original_uni_ids[:,None,None]) < const).argmax(0)]
              
    #return mask
    return out_mask
        



def draw_arrow(image, p, q, color, arrow_magnitude, thickness, line_type, shift):
    # draw arrow tail
    cv2.line(image, p, q, color, thickness, line_type, shift)
    # calc angle of the arrow
    angle = np.arctan2(p[1]-q[1], p[0]-q[0])
    # starting point of first line of arrow head
    p = (int(q[0] + arrow_magnitude * np.cos(angle + np.pi/4)),
    int(q[1] + arrow_magnitude * np.sin(angle + np.pi/4)))
    # draw first half of arrow head
    cv2.line(image, p, q, color, thickness, line_type, shift)
    # starting point of second line of arrow head
    p = (int(q[0] + arrow_magnitude * np.cos(angle - np.pi/4)),
    int(q[1] + arrow_magnitude * np.sin(angle - np.pi/4)))
    # draw second half of arrow head
    cv2.line(image, p, q, color, thickness, line_type, shift)
    
def draw_reg_text(img, obj_info):
    #print 'tbd'
    
    obj_id = obj_info[0]
    cfd = obj_info[1]
    xmin = obj_info[2]
    ymin = obj_info[3]
    xmax = obj_info[4]
    ymax = obj_info[5]

    draw_arrow(img, (xmin, ymin), (xmax, ymin), col_map[obj_id], 0, 5, 8, 0)
    draw_arrow(img, (xmax, ymin), (xmax, ymax), col_map[obj_id], 0, 5, 8, 0)
    draw_arrow(img, (xmax, ymax), (xmin, ymax), col_map[obj_id], 0, 5, 8, 0)
    draw_arrow(img, (xmin, ymax), (xmin, ymin), col_map[obj_id], 0, 5, 8, 0)
    
    # put text
    txt_obj = OBJ_CLASSES[obj_id] + ' ' + str(cfd)
    cv2.putText(img, txt_obj, (xmin, ymin-5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 1) # draw with red
    #cv2.putText(img, txt_obj, (xmin, ymin), cv2.FONT_HERSHEY_SIMPLEX, 1, col_map[obj_id], 2)
    
#     # draw center
#     center_x = (xmax - xmin)/2 + xmin
#     center_y = (ymax - ymin)/2 + ymin
#     cv2.circle(img,(center_x, center_y), 3, (0, 255, 0), -1)
    
    return img



def visualize_mask_asus(im, rois_final, rois_class_score, rois_class_ind, masks, thresh):
    list_bboxes = []
    list_masks = []

    if rois_final.shape[0] == 0:
        print 'No object detection!'
        return list_bboxes, list_masks
    print(rois_class_score[:, -1])
    inds = np.where(rois_class_score[:, -1] >= thresh)[0]
    if len(inds) == 0:
        print 'No detected box with probality > thresh = ', thresh, '-- Choossing highest confidence bounding box.'
        inds = [np.argmax(rois_class_score)]  
        max_conf = np.max(rois_class_score)
        if max_conf < 0.001: 
            return list_bboxes, list_masks, [], []
            
    rois_final = rois_final[inds, :]
    rois_class_score = rois_class_score[inds,:]
    rois_class_ind = rois_class_ind[inds,:]
    
    # get mask
    masks = masks[inds, :, :, :]
    color_curr_mask_list = []

    im_width = im.shape[1]
    im_height = im.shape[0]
    
    # transpose
    im = im[:, :, (2, 1, 0)]

    num_boxes = rois_final.shape[0]
    
    for i in xrange(0, num_boxes):
        
        curr_mask = np.full((im_height, im_width), 0.0, 'float') # convert to int later
            
        class_id = int(rois_class_ind[i,0])
    
        bbox = rois_final[i, 1:5]
        score = rois_class_score[i,0]
        
        if cfg.TEST.MASK_REG:

            x1 = int(round(bbox[0]))
            y1 = int(round(bbox[1]))
            x2 = int(round(bbox[2]))
            y2 = int(round(bbox[3]))

            x1 = np.min((im_width - 1, np.max((0, x1))))
            y1 = np.min((im_height - 1, np.max((0, y1))))
            x2 = np.min((im_width - 1, np.max((0, x2))))
            y2 = np.min((im_height - 1, np.max((0, y2))))
            
            cur_box = [class_id, score, x1, y1, x2, y2]
            list_bboxes.append(cur_box)
            
            h = y2 - y1
            w = x2 - x1
                        
            mask = masks[i, :, :, :]
            mask = np.argmax(mask, axis=0)
            
            original_uni_ids = np.unique(mask)
            
            # sort before_uni_ids and reset [0, 1, 7] to [0, 1, 2]
            original_uni_ids.sort()
            mask = reset_mask_ids(mask, original_uni_ids)
            
            mask = cv2.resize(mask.astype('float'), (int(w), int(h)), interpolation=cv2.INTER_LINEAR)
            #mask = convert_mask_to_original_ids(mask, original_uni_ids)
            mask = convert_mask_to_original_ids_manual(mask, original_uni_ids)
            
            # for mult masks
            curr_mask[y1:y2, x1:x2] = mask 
            
            # visualize each mask
            curr_mask = curr_mask.astype('uint8')
            list_masks.append(curr_mask)
            color_curr_mask = label_colours.take(curr_mask, axis=0).astype('uint8')
            color_curr_mask_list.append(color_curr_mask)
            #cv2.imwrite('mask'+str(i)+'.jpg', color_curr_mask)
            

    img_org = im.copy()
    for ab in list_bboxes:
        print 'box: ', ab
        img_out = draw_reg_text(img_org, ab)



    return list_bboxes, list_masks, color_curr_mask_list, img_out


def get_list_centroid(current_mask, obj_id):
    list_uni_ids = list(np.unique(current_mask))
    list_uni_ids.remove(0) ## remove background id
    print list_uni_ids
    print obj_id
    
    list_centroid = []  ## each row is: obj_id, mask_id, xmean, ymean
    list_allIndex = [] 
    for val in list_uni_ids:
        inds = np.where(current_mask == val) 
        x_index = inds[1]
        y_index = inds[0]
        
        xmean = int(np.mean(x_index))
        ymean = int(np.mean(y_index))
        
        cur_centroid = [obj_id, val, xmean, ymean]
        cur_allIndex = [obj_id, val, x_index, y_index]
        list_centroid.append(cur_centroid)
        list_allIndex.append(cur_allIndex)
        
    return list_centroid, list_allIndex   


def get_list_centroid_and_class():
    fName = "tmp_data/agnostic_objdetection/list_class_center.txt"
    list_class_name = []
    list_class_center = []

    if os.path.exists(fName):
        with open(fName, "r") as f:
            for line in f:
                class_name, center_x, center_y = line.split(' ')
                list_class_name.append(class_name)
                list_class_center.append((float(center_x), float(center_y)))
    else:
        print 'no file for object classes and centers exists'

    return list_class_center, list_class_name

def find_nearest_class_center_for_box(list_boxes, list_class_center, list_class_name):
    list_box2class = []
    for i, box in enumerate(list_boxes):
        dist = float('inf')
        className = None
        for j, center in enumerate(list_class_center):
            cur_dist = np.linalg.norm(np.asarray(center) - [box[2] / 2 + box[4] / 2, box[3] / 2 + box[5] / 2])
            if cur_dist < dist:
                dist = cur_dist
                className = list_class_name[j]
        list_box2class.append(className)
    return list_box2class



# This extended function binarize found objectness into (1) target (2) tool
# For simplicity, target is temporarily defined as objectness with only "GRASPABLE" affordance

def get_list_centroid_and_binary(current_mask, obj_name):
    list_uni_ids = list(np.unique(current_mask))
    list_uni_ids.remove(0) ## remove background id

    #IS_TARGET = False
    #if len(np.where(current_mask == 1)[0]) > len(np.where(current_mask == 4)[0]):
    #    IS_TARGET = True

    
    list_centroid = []  ## each row is: obj_id, mask_id, xmean, ymean
    list_allIndex = [] 
    for val in list_uni_ids:
        inds = np.where(current_mask == val) 
        x_index = inds[1]
        y_index = inds[0]
        
        xmean = int(np.mean(x_index))
        ymean = int(np.mean(y_index))
        
        
        # add condition to filter out noise (total mask region less than threshold)
        threshold = 20
        print("filtering out mask region less than threshold" + str(threshold))
        print("x_index len")
        print len(x_index)
        if len(x_index) < threshold:
            continue

        cur_centroid = [CLASSES.index(obj_name), val, xmean, ymean]
        cur_allIndex = [CLASSES.index(obj_name), val, x_index, y_index]

        #remap class label: 1 to target, 2 to tool
        #if IS_TARGET:
        #    cur_centroid = [1, val, xmean, ymean]
        #    cur_allIndex = [1, val, x_index, y_index]
        #else:
        #    cur_centroid = [2, val, xmean, ymean]
        #    cur_allIndex = [2, val, x_index, y_index]

        list_centroid.append(cur_centroid)
        list_allIndex.append(cur_allIndex)
        
    return list_centroid, list_allIndex  


def convert_bbox_to_centroid(list_boxes, list_masks):
    assert len(list_boxes) == len(list_masks), 'ERROR: len(list_boxes) and len(list_masks) must be equal'
    list_final = []
    list_final_index = []

    list_class_center, list_class_name = get_list_centroid_and_class()
    list_box2class = find_nearest_class_center_for_box(list_boxes, list_class_center, list_class_name)

    for i in range(len(list_boxes)):
        obj_id = list_boxes[i][0] 
        list_centroids, list_allIndex = get_list_centroid_and_binary(list_masks[i], list_box2class[i])  # return [[obj_id, mask_id, xmean, ymean]]
        if len(list_centroids) > 0:
            for l in list_centroids:
                list_final.append(l)
        if len(list_allIndex) > 0:
            for l in list_allIndex:
                list_final_index.append(l)
                
    return list_final, list_final_index


def select_object_and_aff(list_obj_centroids, obj_id, aff_id):
    # select the first object with object id and aff id
    selected_obj_aff = []
    for l in list_obj_centroids:
        if len(l) > 0:
            if l[0] == obj_id and l[1] == aff_id:
                selected_obj_aff.append(l)
                break
    
    selected_obj_aff = selected_obj_aff[0]
    return selected_obj_aff  

    
def project_to_3D(width_x, height_y, depth):
    X = (width_x - KINECT_CX) * depth / KINECT_FX
    Y = (height_y - KINECT_CY) * depth / KINECT_FY
    Z = depth
    p3D = [X, Y, Z]
    
    return p3D

def run_affordance_net_asus(net, im):
    tmp_g = im[:,:,1]
    im[:,:,1] = im[:,:,2] 
    im[:,:,2] = tmp_g  
    #img = im.astype('uint8')
    # Detect all object classes and regress object bounds
    timer = Timer()
    timer.tic()
    if cfg.TEST.MASK_REG:
        rois_final, rois_class_score, rois_class_ind, masks, scores, boxes = im_detect2(net, im)
    else:
        1
    timer.toc()
    print ('Detection took {:.3f}s for '
           '{:d} object proposals').format(timer.total_time, rois_final.shape[0])
    
    # Visualize detections for each class
    return visualize_mask_asus(im, rois_final, rois_class_score, rois_class_ind, masks, thresh=CONF_THRESHOLD)


def parse_args():
    """Parse input arguments."""
    parser = argparse.ArgumentParser(description='AffordanceNet demo')
    parser.add_argument('--gpu', dest='gpu_id', help='GPU device id to use [0]',
                        default=0, type=int)
    parser.add_argument('--cpu', dest='cpu_mode',
                        help='Use CPU mode (overrides --gpu)',
                        action='store_true')
    parser.add_argument('--sim', help='Use simulated data',
                        action='store_true')

    args = parser.parse_args()

    return args


def get_rotation_angle(width, height):
    # find a line to fit
    z = np.polyfit(width, 480 - height, 1)  # y[k] = z[1] + z[0]*x[k]
    if np.var(width) < 90:
        rot = 0.0
    else:
        rot = math.atan(z[0]) / 3.14 * 180 + 90

    print('rot = '+str(rot))
    return rot

def get_coords_from_image_index(width, height):
    # If the object with the correct affordance was found, add the pose to the action message
    print(width, height)
    depth = arr_depth[height, width].astype(float) / 1000

    # trim depth==0
    zeros_index = np.where(depth == 0)[0]
    width, height, depth = np.delete(width, zeros_index, None), np.delete(height, zeros_index, None), np.delete(
        depth, zeros_index, None)
    width_mean, height_mean, depth_mean = np.mean(width), np.mean(height), np.mean(depth)

    # find the full 3D coordinates for highest point
    coords_cam_full = np.concatenate(
        project_to_3D(np.expand_dims(width, 0), np.expand_dims(height, 0), np.expand_dims(depth, 0)), axis=0)
    coords_hom_full = np.concatenate([coords_cam_full, np.ones((1, len(width)))], axis=0)  # 4xN
    coords_3D_full = np.dot(camera_to_marker, coords_hom_full)
    coords_3D_max = np.asarray([np.mean(coords_3D_full[0]), np.mean(coords_3D_full[1]), np.max(coords_3D_full[2])])

    # TODO: check for null depth (?)
    # Convert camera frame coordinates to marker frame
    coords_cam = project_to_3D(width_mean, height_mean, depth_mean)
    coords_3D = np.dot(camera_to_marker, np.append(coords_cam, 1))
    print("coords_3D = " + str(coords_3D))
    print("coords_3D_max = " + str(coords_3D_max))
    # Add the pose attribute to the action message (+0.1, +0.28, -0.12 are used to transform relative to handy)
    return coords_3D, coords_3D_max


if __name__ == '__main__':
    cfg.TEST.HAS_RPN = True  # Use RPN for proposals

    args = parse_args()

    # kl version has bug with landan's PC for unknown reason
    #prototxt = root_path + '/models/pascal_voc/VGG16/faster_rcnn_end2end/test_2nd.prototxt'
    #caffemodel = root_path + '/pretrained/vgg16_faster_rcnn_iter_151000_instance_kl.caffemodel'

    # below trained on UMD
    #prototxt = root_path + '/models/pascal_voc/VGG16/faster_rcnn_end2end/test_umd.prototxt'
    #caffemodel = root_path + '/pretrained/UMD20000.caffemodel'

    # below trained on UMD with instance split
    prototxt = root_path + '/models/pascal_voc/VGG16/faster_rcnn_end2end/test_2nd.prototxt'
    caffemodel = root_path + '/pretrained/vgg16_faster_rcnn_iter_153000_instance_objectness.caffemodel'

    if not os.path.isfile(caffemodel):
        raise IOError(('{:s} not found.\n').format(caffemodel))

    if args.cpu_mode:
        caffe.set_mode_cpu()
    else:
        caffe.set_mode_gpu()
        caffe.set_device(args.gpu_id)
        cfg.GPU_ID = args.gpu_id

    # load network
    net = caffe.Net(prototxt, caffemodel, caffe.TEST)
    print '\n\nLoaded network {:s}'.format(caffemodel)

    # Load Kinect data
    print("Waiting for Kinect data...")
    if args.sim and os.path.exists('tmp_data/agnostic_objdetection/rgb.npy') and os.path.exists('tmp_data/agnostic_objdetection/depth.npy'):
        arr_rgb = np.load('tmp_data/agnostic_objdetection/rgb.npy')
        arr_depth = np.load('tmp_data/agnostic_objdetection/depth.npy')

        list_boxes, list_masks, color_curr_mask_list, img_out = run_affordance_net_asus(net, arr_rgb)

        # display masks
        for i, color_curr_mask in enumerate(color_curr_mask_list):
            cv2.imshow('Mask' + str(i), color_curr_mask)

        # display detection from affordance net
        cv2.imshow('Obj Detection', img_out)

        # break when 'enter' is pressed
        c = cv2.waitKey(0)

        #arr_rgb_show = np.concatenate((arr_rgb[:, :, 2:], arr_rgb[:, :, 1:2], arr_rgb[:, :, 0:1]), axis=2)
        #import scipy.misc
        #scipy.misc.imsave('pliers2bowl.png', arr_rgb_show)
        #plt.imshow(arr_rgb_show)
        #plt.show()
    else:
        # Get image data from kinect
        #rgb =  rospy.wait_for_message("/camera/rgb/image_color", Image)
        #depth = rospy.wait_for_message("/camera/depth_registered/image_raw", Image)

        # Convert ROS image to OpenCV
        #bridge = cv_bridge.CvBridge()
        #rgb = bridge.imgmsg_to_cv2(rgb, desired_encoding="passthrough")
        #depth = bridge.imgmsg_to_cv2(depth, desired_encoding="passthrough")


        while 1:
            # get a frame from RGB camera
            arr_rgb = get_video()
            # get a frame from depth sensor
            depth, arr_depth = get_depth()

            # display RGB image
            cv2.imshow('RGB image', arr_rgb)
            # display depth imagec
            cv2.imshow('Depth image', depth)

            if arr_rgb != []:
                print '##########################################################'
                #np.save('rgb_video.npy', frame_current)
                #import scipy.io as sio
                #sio.savemat('rgb_video.mat', {'rgb_video':frame_current})
                #sio.savemat('depth_video.mat', {'depth_video':depth})

                print '-------------------n-------------------------------------'
                list_boxes, list_masks, color_curr_mask_list, img_out = run_affordance_net_asus(net, arr_rgb)

            # display masks
            if color_curr_mask_list == [] or img_out == []:
                continue
            for i, color_curr_mask in enumerate(color_curr_mask_list):
                cv2.imshow('Mask' + str(i), color_curr_mask)

            # display detection from affordance net
            cv2.imshow('Obj Detection', img_out)

            # break when 'q' is pressed
            c = cv2.waitKey(10)
            if c == ord('q'):
                break

            # save a version for python debugging
            np.save('tmp_data/agnostic_objdetection/rgb.npy', arr_rgb)
            np.save('tmp_data/agnostic_objdetection/depth.npy', arr_depth)
            # save a version for matlab debugging
            import scipy.io as sio
            sio.savemat('tmp_data/agnostic_objdetection/rgb.mat', {'rgb': arr_rgb})
            sio.savemat('tmp_data/agnostic_objdetection/depth.mat', {'depth': arr_depth})





    # Get camera to aruco marker transformation
    marker_to_camera = aruco_camPose(arr_rgb)
    camera_to_marker = np.linalg.inv(marker_to_camera)

    if (arr_rgb.shape[0] > 100 and arr_rgb.shape[1] > 100):
        print '-------------------n-------------------------------------'
        #list_boxes, list_masks = run_affordance_net_asus(net, arr_rgb)
        print 'len list boxes: ', len(list_boxes)
        list_obj_centroids, list_obj_centroids_index = convert_bbox_to_centroid(list_boxes, list_masks)
        print list_obj_centroids

        # Writes the problem .pddl file and is located at lib/utils/handy.py
        write_pddl('../pddl/', list_obj_centroids, loadstate)

        # Generate the action plan using the fast-downward python script (alternative?)
        subprocess.call('~/fast-downward/fast-downward.py ../pddl/domain_CONTAINABLE.pddl ../pddl/auto_problem_CONTAINABLE_STATE.pddl --search "astar(blind())"', shell=True)

        # Parse the plan generated by fast downward into a list of actions and objects
        plan = []
        with open('./sas_plan') as f:
            for i, line in enumerate(f.readlines()[:-1]):
                action = []
                # List of words in the current line of the sas_plan
                elements = line.split(' ')

                # Extract the action and convert it into an integer using the hashmap ACTION_INDS
                action_ind = ACTION_INDS[elements[0][1:]]
                action.append(action_ind)

                # Iterate through objects for the action
                for element in elements:
                    # Remove last parantheses (there is probably an easier way)
                    obj = element.split(')')[0]

                    # Map the object to an integer if it is in the list of possible objects
                    if obj in OBJ_INDS:
                        action.append(OBJ_INDS[obj])

                plan.append(action)
    print("plan: ")
    print(plan)

    # Next few lines converts pixel coordinates to marker frame world coordinates for Rviz visualization
    width, height = arr_rgb.shape[:2]
    rows = np.arange(0, height)
    cols = np.arange(0, width)
    x, y = np.meshgrid(rows, cols)

    # Pixel coordinates with depth
    coords = np.concatenate([x.reshape(-1, 1), y.reshape(-1, 1), arr_depth.reshape(-1, 1)], axis=1).T.astype(float) # Nx3

    # Convert the pixel coordinates to 3D coordinates
    coords_3D = np.concatenate(project_to_3D(coords[np.newaxis, 0], coords[np.newaxis, 1], coords[np.newaxis, 2]), axis=0) # 3xN

    # Make homogeneous coordinates (concatenate row of 1s)
    coords_hom = np.concatenate([coords_3D/1000, np.ones((1, width*height))], axis=0) # 4xN

    # Convert coordinates from camera from into the marker frame
    coords_cam = np.dot(camera_to_marker, coords_hom) # 4xN

    # Replace 1s with rgb values
    arr_rgb = arr_rgb.astype(int)

    # following visualization of the point cloud, for efficiency, downsampling 100x
    #from matplotlib import pyplot
    #from mpl_toolkits.mplot3d import Axes3D

    #fig = pyplot.figure()
    #ax = Axes3D(fig)

    #ax.scatter(coords_cam[0][::100], coords_cam[1][::100], coords_cam[2][::100])
    #pyplot.show()

    # Add color to point cloud for Rviz (does not work!!)
    red = np.left_shift(arr_rgb[:, :, 0].reshape(-1), 16)
    green = np.left_shift(arr_rgb[:, :, 1].reshape(-1), 8)
    blue = arr_rgb[:, :, 2].reshape(-1)
    coords_cam[-1] = red + green + blue

    # Create point cloud 2 message
    fields = [PointField('x', 0, PointField.FLOAT32, 1),
              PointField('y', 4, PointField.FLOAT32, 1),
              PointField('z', 8, PointField.FLOAT32, 1),
              PointField('rgb', 16, PointField.FLOAT32, 1)]
    h = msg.Header()
    h.stamp = rospy.Time.now()
    h.frame_id = '/affordance_net'
    pc2_msg = pc2.create_cloud(h, fields, coords_cam.T)

    # Create an action message and populate its fields
    action_list = action_msg()
    action_list.header.frame_id = "camera_depth_optical_frame"

    # read state from previous move
    import collections
    state_obj_location_list = collections.defaultdict()
    if loadstate:
        f = open('pddl_state_obj_location', 'r')
        lines = f.readlines()
        for line in lines:
            obj_location = line.split(' ')
            state_obj_location_list[obj_location[0]] = (float(obj_location[1]), float(obj_location[2]), float(obj_location[3]))

    for action in plan:
        action_list.action.append(action[0])

        # Iterate through objects
        offset, offset_max = None, None # this is for scooping offset (grasp center to scoop center)
        dropoff_obj_id = None
        for arg, obj in enumerate(action[1:]):
            width, height = [], []

            # If the action is pickup, select desired object centroid based on grasp (9)
            if action[0] == 0:
                if str(obj) in state_obj_location_list:
                    coords_3D_max = np.asarray([state_obj_location_list[str(obj)][0], state_obj_location_list[str(obj)][1], state_obj_location_list[str(obj)][2]])
                    coords_3D = coords_3D_max

                    width, height = [0], [0]
                    angle = 0
                else:
                    #_, _, width, height = select_object_and_aff(list_obj_centroids, obj, 9)
                    _, _, width, height = select_object_and_aff(list_obj_centroids_index, obj, 1)
                    coords_3D, coords_3D_max = get_coords_from_image_index(width, height)

                    # find angle for grasping
                    angle = get_rotation_angle(width, height)

            # If the action is dropoff, select desired object centroid based on contains (1)
            elif action[0] == 1:
                # keep dropoff object id
                if arg == 0:
                    dropoff_obj_id = obj
                    continue

                # check if it is the 2 argument (first is irrelevant, review .pddl files for info)
                if arg == 1:
                    #_, _, width, height = select_object_and_aff(list_obj_centroids, obj, 1)
                    _, _, width, height = select_object_and_aff(list_obj_centroids_index, obj, 4)
                    coords_3D, coords_3D_max = get_coords_from_image_index(width, height)
                    angle = 0

                    with open('pddl_state_obj_location', 'w') as f:
                        f.write(str(dropoff_obj_id)+' '+str(coords_3D_max[0])+' '+str(coords_3D_max[1])+' '+str(coords_3D_max[2]))

            # If the action is scoop, select desired object centroid based on contains (1)
            elif action[0] == 2:
                # check if it is the 2 argument (first is irrelevant, review .pddl files for info)
                if arg == 0:
                    # check grasp center
                    _, _, width, height = select_object_and_aff(list_obj_centroids_index, obj, 1)
                    coords_3D_grasp, coords_3D_max_grasp = get_coords_from_image_index(width, height)

                    # check scoop center
                    _, _, width, height = select_object_and_aff(list_obj_centroids_index, obj, 3)
                    coords_3D_scoop, coords_3D_max_scoop = get_coords_from_image_index(width, height)

                    offset, offset_max = np.linalg.norm(coords_3D_grasp-coords_3D_scoop), np.linalg.norm(coords_3D_max_grasp-coords_3D_max_scoop)

                    continue

                if arg == 1:
                    #_, _, width, height = select_object_and_aff(list_obj_centroids, obj, 1)
                    _, _, width, height = select_object_and_aff(list_obj_centroids_index, obj, 6) # for bowl with beans => supportable: 6
                    coords_3D, coords_3D_max = get_coords_from_image_index(width, height)
                    if offset and offset_max:
                        coords_3D[1], coords_3D_max[1] = coords_3D[1]-offset, coords_3D_max[1]-offset_max
                    angle = 0

            # # If the object with the correct affordance was found, add the pose to the action message
            if len(width) != 0 and len(height) != 0:
            #     print(width, height)
            #     depth = arr_depth[height, width].astype(float) / 1000
            #
            #     # trim depth==0
            #     zeros_index = np.where(depth == 0)[0]
            #     width, height, depth = np.delete(width, zeros_index, None), np.delete(height, zeros_index, None), np.delete(depth, zeros_index, None)
            #     width_mean, height_mean, depth_mean = np.mean(width), np.mean(height), np.mean(depth)
            #
            #     # find the full 3D coordinates for highest point
            #     coords_cam_full = np.concatenate(project_to_3D(np.expand_dims(width, 0), np.expand_dims(height, 0), np.expand_dims(depth, 0)), axis=0)
            #     coords_hom_full = np.concatenate([coords_cam_full, np.ones((1, len(width)))], axis=0)  # 4xN
            #     coords_3D_full = np.dot(camera_to_marker, coords_hom_full)
            #     coords_3D_max = [np.mean(coords_3D_full[0]), np.mean(coords_3D_full[1]), np.max(coords_3D_full[2])]
            #
            #     # TODO: check for null depth (?)
            #     # Convert camera frame coordinates to marker frame
            #     coords_cam = project_to_3D(width_mean, height_mean, depth_mean)
            #     coords_3D = np.dot(camera_to_marker, np.append(coords_cam, 1))
            #     print("coords_3D = " + str(coords_3D))
            #     print("coords_3D_max = " + str(coords_3D_max))
            #     # Add the pose attribute to the action message (+0.1, +0.28, -0.12 are used to transform relative to handy)

                obj_pose_3D = Pose()
                obj_pose_3D.position.x = round(coords_3D_max[0], 2) + 0.30
                obj_pose_3D.position.y = round(coords_3D_max[1], 2) + 0.32
                obj_pose_3D.position.z = round(coords_3D_max[2], 2) - 0.0178 - 0.045
                obj_pose_3D.orientation.x = 0
                obj_pose_3D.orientation.y = 0
                obj_pose_3D.orientation.z = 0
                obj_pose_3D.orientation.w = angle

                action_list.pose.append(obj_pose_3D)

    # Print out the published message
    print(action_list)
    print("action_list printed")

    # Continously publish the point cloud for Rviz and an action message for pickplace_kinect.cpp
    try:
      while(1):
        pub_point_cloud.publish(pc2_msg)
        pub_action_msg.publish(action_list)
        time.sleep(10)

    except KeyboardInterrupt:
        print('interrupted!')

